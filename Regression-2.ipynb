{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca545c-a01d-4b4c-85aa-f263e7b12f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "\n",
    "ANS-1\n",
    "\n",
    "\n",
    "R-squared (R2) is a statistical metric used to evaluate the goodness of fit of a linear regression model. It represents the proportion of the variance in the dependent variable (response variable) that is explained by the independent variable(s) (predictor variable(s)) included in the model. In other words, R-squared measures how well the independent variable(s) can account for the variation in the dependent variable.\n",
    "\n",
    "R-squared values range from 0 to 1, with:\n",
    "- R2 = 0: The model explains none of the variance in the dependent variable. It indicates that the model's predictions are no better than using the mean of the dependent variable as the prediction for all data points.\n",
    "- R2 = 1: The model explains 100% of the variance in the dependent variable. It indicates that the model perfectly fits the data, and all variations in the dependent variable can be explained by the independent variable(s).\n",
    "\n",
    "Calculating R-squared:\n",
    "To calculate R-squared, follow these steps:\n",
    "\n",
    "1. Fit the linear regression model to the data and obtain the predicted values ŷ for each data point.\n",
    "\n",
    "\n",
    "2. Calculate the mean of the observed dependent variable (ȳ) and the total sum of squares (SST) by finding the sum of the squared differences between each observed y value and the mean ȳ.\n",
    "   SST = ∑(y - ȳ)^2, where y is the observed value and ȳ is the mean of y.\n",
    "3. Calculate the residual sum of squares (SSE) by finding the sum of the squared differences between each observed y value and the corresponding predicted ŷ value.\n",
    "   SSE = ∑(y - ŷ)^2, where y is the observed value and ŷ is the predicted value.\n",
    "4. Calculate R-squared using the formula:\n",
    "   R2 = 1 - (SSE / SST)\n",
    "\n",
    "Interpreting R-squared:\n",
    "- R-squared close to 0: The model explains little to no variance in the dependent variable, and the model's predictions are not much better than the mean of the dependent variable.\n",
    "- R-squared close to 1: The model explains a large proportion of the variance in the dependent variable, and the model's predictions align well with the actual data points.\n",
    "\n",
    "However, it's essential to note that R-squared should not be the sole metric used to evaluate the goodness of fit. It has limitations, especially when dealing with complex datasets and multiple predictors. R-squared can increase when additional predictors are added to the model, even if those predictors do not contribute significantly to the model's predictive power. Thus, it is crucial to use other evaluation metrics and consider the context of the problem when assessing the performance of a linear regression model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "\n",
    "ANS-2\n",
    "\n",
    "\n",
    "Adjusted R-squared is a modified version of the regular R-squared (R2) used in linear regression models. While R-squared measures the proportion of variance in the dependent variable explained by the independent variable(s), adjusted R-squared takes into account the number of predictors (independent variables) in the model. It penalizes the inclusion of irrelevant or less significant predictors, providing a more balanced assessment of the model's goodness of fit.\n",
    "\n",
    "The formula for calculating adjusted R-squared is:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R2) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "Where:\n",
    "- R2 is the regular R-squared value.\n",
    "- n is the number of data points (sample size).\n",
    "- k is the number of predictors (independent variables) in the model.\n",
    "\n",
    "Differences between R-squared and adjusted R-squared:\n",
    "\n",
    "1. **Inclusion of Predictors**:\n",
    "   - R-squared: R2 does not consider the number of predictors in the model. It only measures the proportion of variance in the dependent variable explained by the predictors.\n",
    "   - Adjusted R-squared: It takes into account the number of predictors in the model through the \"penalty\" term (n - k - 1). The penalty increases with the number of predictors, providing a more conservative evaluation of the model's fit.\n",
    "\n",
    "2. **Effect of Adding Predictors**:\n",
    "   - R-squared: When additional predictors are added to the model, R2 will never decrease. It may remain the same or increase, even if the additional predictors do not contribute significantly to explaining the variance in the dependent variable.\n",
    "   - Adjusted R-squared: It increases only when adding relevant predictors that significantly improve the model's fit. If a new predictor does not contribute enough to the model, the increase in R2 will be offset by the penalty term, leading to a decrease in the adjusted R-squared.\n",
    "\n",
    "3. **Interpretability**:\n",
    "   - R-squared: R2 is often interpreted as the proportion of variance explained by the model, ranging from 0 to 1. Higher R2 values are preferred, indicating a better fit.\n",
    "   - Adjusted R-squared: It penalizes the model for including unnecessary predictors, leading to a value that may be lower than R2. Adjusted R-squared is considered a more reliable metric when comparing models with different numbers of predictors.\n",
    "\n",
    "In summary, adjusted R-squared is a more robust evaluation metric for linear regression models, especially when dealing with models with varying numbers of predictors. It discourages overfitting and provides a better measure of the model's ability to generalize to new data by considering the trade-off between model complexity and goodness of fit. However, it is essential to use both R-squared and adjusted R-squared, along with other evaluation techniques, to gain a comprehensive understanding of the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "\n",
    "ANS-3\n",
    "\n",
    "\n",
    "Adjusted R-squared is more appropriate to use in situations where you have multiple predictor variables (independent variables) in a linear regression model. It is particularly useful when you want to assess the model's goodness of fit while considering the trade-off between model complexity and the number of predictors.\n",
    "\n",
    "Here are some situations when using adjusted R-squared is more appropriate:\n",
    "\n",
    "1. **Comparing Models with Different Numbers of Predictors**: When comparing multiple linear regression models with varying numbers of predictors, adjusted R-squared provides a fairer comparison. It penalizes the inclusion of unnecessary predictors and helps identify the model that achieves the best balance between explanatory power and simplicity.\n",
    "\n",
    "2. **Avoiding Overfitting**: Adjusted R-squared helps in avoiding overfitting, which occurs when the model is too complex and fits the training data too closely, but performs poorly on new, unseen data. By penalizing the model for including less relevant predictors, it discourages overfitting and encourages more generalizable models.\n",
    "\n",
    "3. **Complex Models**: In cases where you have a large number of potential predictors to choose from, adjusted R-squared can guide you in selecting the most relevant and significant predictors. It encourages the inclusion of predictors that genuinely improve the model's fit and predictive performance.\n",
    "\n",
    "4. **Small Sample Size**: When dealing with a small sample size, regular R-squared may be overly optimistic in estimating the model's fit. Adjusted R-squared, by taking into account the sample size and the number of predictors, provides a more conservative assessment of the model's performance.\n",
    "\n",
    "5. **Regression with Many Predictors**: In multiple linear regression with many predictors, regular R-squared can be excessively high, even when some predictors have little to no impact on the dependent variable. Adjusted R-squared offers a more cautious estimate of the model's explanatory power.\n",
    "\n",
    "It's important to note that while adjusted R-squared addresses certain limitations of regular R-squared, it is not a perfect measure. It is still essential to use other evaluation techniques and consider the context of the problem when assessing the performance of a linear regression model. Additionally, adjusted R-squared should not be used as the sole criterion for model selection; other factors such as interpretability, domain knowledge, and practicality should also be taken into account\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "\n",
    "\n",
    "ANS-4\n",
    "\n",
    "\n",
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used evaluation metrics in the context of regression analysis. These metrics are used to assess the performance of regression models by measuring the difference between the predicted values and the actual values of the dependent variable.\n",
    "\n",
    "1. **Mean Squared Error (MSE)**:\n",
    "   - MSE is a measure of the average squared difference between the predicted values and the actual values.\n",
    "   - It penalizes larger errors more heavily since it involves squaring the differences.\n",
    "   - The formula for calculating MSE is:\n",
    "     MSE = (1/n) * ∑(y - ŷ)^2\n",
    "     where n is the number of data points, y is the actual value, and ŷ is the predicted value.\n",
    "\n",
    "2. **Root Mean Squared Error (RMSE)**:\n",
    "   - RMSE is the square root of the MSE and represents the average magnitude of the errors between predicted and actual values.\n",
    "   - It is in the same unit as the dependent variable, making it easier to interpret.\n",
    "   - The formula for calculating RMSE is:\n",
    "     RMSE = √(MSE)\n",
    "\n",
    "3. **Mean Absolute Error (MAE)**:\n",
    "   - MAE is a measure of the average absolute difference between the predicted values and the actual values.\n",
    "   - It is less sensitive to outliers compared to MSE because it does not involve squaring the differences.\n",
    "   - The formula for calculating MAE is:\n",
    "     MAE = (1/n) * ∑|y - ŷ|\n",
    "     where n is the number of data points, y is the actual value, and ŷ is the predicted value.\n",
    "\n",
    "Interpretation of the Metrics:\n",
    "- All three metrics, RMSE, MSE, and MAE, provide information about the accuracy of the regression model's predictions.\n",
    "- Lower values of RMSE, MSE, and MAE indicate better model performance, with smaller errors between predicted and actual values.\n",
    "- RMSE is often preferred when the magnitudes of the errors are significant, as it squares the errors, giving more weight to larger errors.\n",
    "- MAE is useful when the data contains outliers, as it is less sensitive to extreme values compared to MSE.\n",
    "\n",
    "Choosing the appropriate metric depends on the specific context and goals of the regression analysis. While RMSE and MSE are commonly used in many applications, MAE may be preferred in situations where outliers have a considerable impact on the model's performance or when the magnitude of errors is of particular interest. It's essential to use multiple evaluation metrics and consider the practical implications of the model's performance when interpreting and comparing the results.\n",
    "\n",
    "\n",
    "\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\n",
    "\n",
    "ANS-5\n",
    "\n",
    "\n",
    "Advantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis:\n",
    "\n",
    "1. **Easy Interpretation**: RMSE, MSE, and MAE are intuitive and easy to understand evaluation metrics. They provide a straightforward measure of the model's accuracy by quantifying the difference between predicted and actual values.\n",
    "\n",
    "2. **Sensitivity to Errors**: These metrics penalize larger errors, which is essential in many real-world applications where large errors can have more significant consequences or impact on decision-making.\n",
    "\n",
    "3. **Continuous Scale**: RMSE, MSE, and MAE produce continuous values that can be compared across different models or different datasets, allowing for objective model selection and performance comparison.\n",
    "\n",
    "4. **Implementation**: These metrics are easy to compute, making them computationally efficient for large datasets.\n",
    "\n",
    "5. **Model Tuning**: RMSE, MSE, and MAE can be used for hyperparameter tuning in regression models, helping to select the best combination of parameters that result in the lowest error.\n",
    "\n",
    "Disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis:\n",
    "\n",
    "1. **Outlier Sensitivity**: RMSE and MSE are more sensitive to outliers than MAE because they involve squaring the errors. Outliers can disproportionately influence the metrics and lead to potential inaccuracies in model evaluation.\n",
    "\n",
    "2. **Unit Dependency**: RMSE and MSE are sensitive to the scale of the dependent variable, as they involve squared errors. This makes it challenging to compare models when the dependent variable is measured in different units.\n",
    "\n",
    "3. **Focus on Large Errors**: While penalizing larger errors can be advantageous, it may not always be the most critical aspect of the model's performance, especially when small errors are more critical in specific applications.\n",
    "\n",
    "4. **Infinite Value**: RMSE and MSE can become infinite when the model has poor performance and produces predictions that are significantly different from the actual values. This can make it difficult to interpret and compare models with extreme errors.\n",
    "\n",
    "5. **Lack of Directionality**: RMSE, MSE, and MAE do not provide information about the direction of errors. They only quantify the magnitude of the errors but do not indicate whether the model is overestimating or underestimating the actual values.\n",
    "\n",
    "In summary, RMSE, MSE, and MAE are valuable evaluation metrics in regression analysis, providing insights into the accuracy of the model's predictions. However, they each have their strengths and weaknesses, and the choice of the appropriate metric should be based on the specific characteristics of the dataset, the goals of the analysis, and the context of the problem. It\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "\n",
    "\n",
    "ANS-6\n",
    "\n",
    "\n",
    "\n",
    "Lasso regularization, also known as L1 regularization, is a technique used in linear regression and other linear models to prevent overfitting and improve model performance. It adds a penalty term to the model's cost function that is proportional to the absolute values of the model's coefficients. The goal of Lasso regularization is to encourage the model to reduce the magnitude of less important or irrelevant features by driving their corresponding coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "The Lasso regularization term is added to the standard linear regression cost function (mean squared error) and can be represented as:\n",
    "\n",
    "Lasso Regularization Term = α * ∑|βi|\n",
    "\n",
    "Where:\n",
    "- α (alpha) is the regularization parameter that controls the strength of the regularization. A larger α value increases the penalty on the coefficients and results in more coefficients being pushed to zero.\n",
    "- βi are the coefficients of the model.\n",
    "\n",
    "Differences between Lasso and Ridge regularization:\n",
    "\n",
    "1. **Regularization Term**:\n",
    "   - Lasso Regularization: The penalty term in Lasso regularization is proportional to the absolute values of the coefficients.\n",
    "   - Ridge Regularization: The penalty term in Ridge regularization is proportional to the squared values of the coefficients.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - Lasso Regularization: Lasso has a feature selection property, as it tends to set the coefficients of less important features to exactly zero. This leads to a sparse model with only the most relevant features.\n",
    "   - Ridge Regularization: Ridge tends to shrink the coefficients towards zero but does not eliminate any feature entirely. It does not perform feature selection as effectively as Lasso.\n",
    "\n",
    "3. **Number of Selected Features**:\n",
    "   - Lasso Regularization: Lasso can lead to a model with fewer features, making it more interpretable and potentially reducing the risk of overfitting in high-dimensional datasets.\n",
    "   - Ridge Regularization: Ridge may retain more features, as the coefficients are reduced but not set to exactly zero.\n",
    "\n",
    "When to use Lasso regularization:\n",
    "\n",
    "Lasso regularization is more appropriate to use in the following situations:\n",
    "\n",
    "1. **Feature Selection**: When dealing with datasets that have many irrelevant or less important features, Lasso can effectively select the most relevant features and eliminate the less useful ones, leading to a simpler and more interpretable model.\n",
    "\n",
    "2. **Sparse Solutions**: If you suspect that only a few features have a significant impact on the dependent variable, Lasso can help identify those important features and build a sparse model.\n",
    "\n",
    "3. **High-Dimensional Data**: Lasso is particularly useful in high-dimensional datasets, where the number of features is larger than the number of data points. It can help prevent overfitting and improve generalization in such cases.\n",
    "\n",
    "4. **Interpretability**: Lasso can provide a more interpretable model with fewer features, making it suitable when interpretability is a priority.\n",
    "\n",
    "It's important to note that the choice between Lasso and Ridge regularization depends on the specific characteristics of the data and the problem at hand. Sometimes a combination of both Lasso and Ridge regularization, known as Elastic Net regularization, may be more suitable, as it combines the benefits of both techniques and offers a balanced solution. Cross-validation and model evaluation on validation data can help determine the most appropriate regularization technique for a given problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "\n",
    "ANS-7\n",
    "\n",
    "\n",
    "Regularized linear models, such as Ridge regression and Lasso regression, help prevent overfitting in machine learning by introducing a penalty term to the model's cost function that discourages the coefficients from taking large values. This penalty term limits the model's complexity and controls the influence of individual features, thus reducing the risk of overfitting.\n",
    "\n",
    "Overfitting occurs when a model learns the noise and random fluctuations in the training data rather than the true underlying patterns. As a result, the model performs well on the training data but poorly on new, unseen data. Regularization techniques combat overfitting by imposing constraints on the model's coefficients during the training process.\n",
    "\n",
    "Let's illustrate this with an example:\n",
    "\n",
    "Suppose we have a dataset of housing prices, where we want to predict the price (dependent variable) based on various features such as area, number of bedrooms, and location. We decide to use a polynomial regression model to capture any nonlinear relationships between the features and the price.\n",
    "\n",
    "1. **Without Regularization (Ordinary Polynomial Regression)**:\n",
    "We fit a high-degree polynomial regression model (e.g., 10th-degree polynomial) to the training data. This model can perfectly fit the training data, as it has enough flexibility to pass through every data point. However, when we evaluate the model on new data (validation or test set), it performs poorly, showing a high level of overfitting. The model has learned noise and specific patterns in the training data that do not generalize well to unseen data.\n",
    "\n",
    "2. **With Regularization (Ridge Regression)**:\n",
    "To prevent overfitting, we use Ridge regression, which adds a penalty term to the cost function based on the squared values of the model's coefficients. This penalty term is controlled by a hyperparameter α (alpha). A higher value of α increases the strength of regularization. As α increases, the model's coefficients are pushed towards zero, and the model becomes more robust and less prone to overfitting.\n",
    "\n",
    "By using Ridge regression, the high-degree polynomial model will be regularized, and the impact of irrelevant or less significant features will be reduced. The resulting model will have smaller coefficients and be less sensitive to variations in the training data, leading to improved generalization to new data.\n",
    "\n",
    "3. **With Regularization (Lasso Regression)**:\n",
    "Alternatively, we can use Lasso regression, which adds a penalty term to the cost function based on the absolute values of the model's coefficients. Like Ridge regression, Lasso can prevent overfitting by driving less important coefficients to exactly zero. This leads to a sparse model with only the most relevant features.\n",
    "\n",
    "By using Lasso regression, the high-degree polynomial model will not only be regularized but also have some of its coefficients set to zero. This results in a more interpretable model with feature selection, as only the most relevant features will remain in the model.\n",
    "\n",
    "In summary, regularized linear models effectively prevent overfitting by controlling the model's complexity and feature selection through the introduction of penalty terms. These techniques help balance the trade-off between fitting the training data well and generalizing to new, unseen data, making the models more robust and reliable in real-world applications.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "\n",
    "\n",
    "ANS-8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "While regularized linear models like Ridge regression and Lasso regression are powerful techniques for preventing overfitting and improving model performance, they have certain limitations and may not always be the best choice for regression analysis. Some of the limitations include:\n",
    "\n",
    "1. **Interpretability**: Regularized linear models, especially Lasso regression, can lead to sparse models with some coefficients being exactly zero. While this feature selection property is beneficial in some cases, it can make the model less interpretable, as some features are entirely excluded from the model, and their effects on the target variable cannot be assessed.\n",
    "\n",
    "2. **Feature Selection Bias**: Lasso regression's feature selection can be biased towards selecting features that may have little predictive power in the population but happen to be useful in the specific training dataset. As a result, the selected features might not generalize well to new data, especially when the training data is limited.\n",
    "\n",
    "3. **Hyperparameter Tuning**: Regularized linear models have hyperparameters (e.g., α in Ridge regression and Lasso regression) that control the strength of the regularization. Selecting the optimal hyperparameters requires cross-validation or other search techniques, which can be computationally expensive and time-consuming.\n",
    "\n",
    "4. **Large-Scale Data**: For large-scale datasets with a substantial number of features, the computational cost of regularized linear models can become prohibitive. Training and optimizing these models on such datasets may be resource-intensive.\n",
    "\n",
    "5. **Non-Linear Relationships**: While regularized linear models can capture some degree of non-linearity through polynomial features, they are still fundamentally linear models. When dealing with datasets that exhibit complex non-linear relationships, other non-linear regression techniques (e.g., decision trees, random forests, or neural networks) may be more appropriate.\n",
    "\n",
    "6. **Incorporating Domain Knowledge**: Regularized linear models are data-driven and may not easily accommodate prior knowledge or domain-specific constraints on the model. In some cases, domain knowledge might suggest specific relationships between variables that are not easily captured by regularization techniques.\n",
    "\n",
    "7. **Outliers**: Lasso regression, in particular, can be sensitive to outliers, as the absolute penalty term can be influenced by large errors in the data. Outliers might disproportionately impact the model's performance and coefficient estimates.\n",
    "\n",
    "8. **Collinearity**: While Ridge regression can handle multicollinearity (high correlation between predictors) relatively well, Lasso regression may have issues when dealing with highly correlated predictors, as it can arbitrarily select one predictor and set the coefficients of the others to zero.\n",
    "\n",
    "In conclusion, while regularized linear models are valuable tools in regression analysis, they are not always the best choice for every situation. Researchers and data analysts should carefully consider the specific characteristics of the data, the interpretability requirements, the complexity of the relationships, and other aspects of the problem before deciding on the appropriate regression technique. In some cases, other regression methods, or even non-linear models, might be more suitable for achieving the best results and insights from the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS-9\n",
    "\n",
    "\n",
    "\n",
    "To choose the better performer between Model A and Model B, we need to consider the specific context of the problem and the goals of the analysis, as each evaluation metric (RMSE and MAE) provides different insights into the model's performance.\n",
    "\n",
    "1. **Root Mean Squared Error (RMSE)**:\n",
    "   - RMSE is the square root of the Mean Squared Error (MSE) and represents the average magnitude of the errors between the predicted and actual values.\n",
    "   - RMSE penalizes larger errors more heavily due to the squaring operation, making it sensitive to outliers.\n",
    "\n",
    "2. **Mean Absolute Error (MAE)**:\n",
    "   - MAE is the average absolute difference between the predicted and actual values.\n",
    "   - MAE is less sensitive to outliers compared to RMSE because it does not involve squaring the errors.\n",
    "\n",
    "Choosing the Better Model:\n",
    "- If the primary concern is to minimize large errors and outliers, Model A (with an RMSE of 10) would be a better choice, as it penalizes larger errors more heavily. This is particularly important when the consequences of large errors are significant or when the application requires accurate prediction of extreme values.\n",
    "\n",
    "- If the dataset contains outliers or extreme values that are crucial to predict accurately, Model B (with an MAE of 8) might be preferred, as it is less influenced by outliers and provides a more robust estimate of the average prediction error.\n",
    "\n",
    "Limitations:\n",
    "It's essential to recognize the limitations of each metric and consider the specific context:\n",
    "\n",
    "- RMSE and MSE are more sensitive to outliers, which might inflate their values and potentially lead to misleading conclusions about the model's performance in the presence of extreme values.\n",
    "\n",
    "- MAE does not account for the magnitude of errors and treats all errors equally. In some cases, this might be desirable, but it may also overlook the importance of larger errors, especially when small errors are more critical.\n",
    "\n",
    "- Both RMSE and MAE do not provide information about the direction of errors. They only measure the magnitude of the errors without indicating whether the model is overestimating or underestimating the actual values.\n",
    "\n",
    "- It is also possible to consider other evaluation metrics, such as Mean Absolute Percentage Error (MAPE) or R-squared (coefficient of determination), which offer additional insights into the model's performance from different perspectives.\n",
    "\n",
    "In summary, the choice of the better model between Model A and Model B depends on the specific requirements and priorities of the problem. It is essential to consider multiple evaluation metrics, examine the distribution of errors, and understand the context of the data to make an informed decision about the model's performance. Additionally, using cross-validation and evaluating the models on unseen data can provide a more reliable assessment of their generalization capabilities.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\n",
    "\n",
    "ANS-10\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
